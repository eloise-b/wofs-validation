{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point-based and Parallel Processing Water Observations from Space (WOfS) Product in Africa Using Sentinel 2  <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:**\n",
    "[s2_l2a](https://explorer.digitalearth.africa/s2_l2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description \n",
    "The [Water Observations from Space (WOfS)](https://www.ga.gov.au/scientific-topics/community-safety/flood/wofs/about-wofs) is a derived product from Landsat 8 satellite observations as part of provisional Landsat 8 Collection 2 surface reflectance and shows surface water detected in Africa.\n",
    "Individual water classified images are called Water Observation Feature Layers (WOFLs), and are created in a 1-to-1 relationship with the input satellite data. \n",
    "Hence there is one WOFL for each satellite dataset processed for the occurrence of water.\n",
    "\n",
    "The data in a WOFL is stored as a bit field. This is a binary number, where each digit of the number is independantly set or not based on the presence (1) or absence (0) of a particular attribute (water, cloud, cloud shadow etc). In this way, the single decimal value associated to each pixel can provide information on a variety of features of that pixel. \n",
    "For more information on the structure of WOFLs and how to interact with them, see [Water Observations from Space](../Datasets/Water_Observations_from_Space.ipynb) and [Applying WOfS bitmasking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebooks.\n",
    "\n",
    "This notebook explains how you can query WOfS product for each collected validation points in Africa based on point-based sampling approach. \n",
    "\n",
    "The notebook demonstrates how to:\n",
    "\n",
    "1. Load validation points for each partner institutions following cleaning stage described in notebook 1\n",
    "2. Query WOFL data for validation points and capture available WOfS defined class using point-based sampling and multiprocessing functionality\n",
    "3. Extract a LUT for each point that contains both information for validation points and WOfS class as well number of clear observation in each month \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "from datacube.utils import masking, geometry \n",
    "import sys\n",
    "import os\n",
    "import rasterio\n",
    "import xarray\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import scipy, scipy.ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #this will suppress the warnings for multiple UTM zones in your AOI \n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score  \n",
    "from deafrica_plotting import map_shapefile,display_map, rgb\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_datahandling import wofs_fuser, mostcommon_crs,load_ard,deepcopy\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse validation points collected by each partner institution, we need to obtain WOfS surface water observation data that corresponds with the labelled input data locations. \n",
    "- Path2csv: the path to CEO validation points labelled by each partner institutions in Africa \n",
    "- ValPoints: CEO validation points labelled by each partner institutions in Africa in ESRI shapefile format \n",
    "- Path: Direct path to the ESRI shapefile in case that the shapefile in available\n",
    "- input_data: geopandas datafram for CEO validation points labelled by each partner institutions in Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Note: Run the following three cells in case that you dont have a ESRI shapefile for validation points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2csv = '../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.csv'\n",
    "df = pd.read_csv(path2csv,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometries = [Point(xy) for xy in zip(df.LON, df.LAT)]\n",
    "crs = {'init': 'epsg:4326'} \n",
    "ValPoints = GeoDataFrame(df, crs=crs, geometry=geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValPoints.to_file(filename='../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.shp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Note: In case that you have ESRI shapefile for validation points, please continute from this point onward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed_ 0', 'PLOT_ID', 'LON', 'LAT', 'FLAGGED', 'ANALYSES',\n",
       "       'SENTINEL2Y', 'STARTDATE', 'ENDDATE', 'WATER', 'NO_WATER', 'BAD_IMAGE',\n",
       "       'NOT_SURE', 'CLASS', 'COMMENT', 'MONTH', 'WATERFLAG', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the table and converting CRS to metric\n",
    "input_data = gpd.read_file(path).to_crs('epsg:6933')  \n",
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= input_data.drop(['Unnamed_ 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8724, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the size of the input data \n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample WOfS at the ground truth coordinates\n",
    "To load WOFL data, we can first create a re-usable query as below that will define two particular items, `group_by` solar day, ensuring that the data between scenes is combined correctly. The second parameter is `resampling` method that is set to be nearest. This query will later be updated in the script for other parameters to conduct WOfS query. the time period we are interested in, as well as other important parameters that are used to correctly load the data. \n",
    "\n",
    "We can convert the WOFL bit field into a binary array containing True and False values. This allows us to use the WOFL data as a mask that can be applied to other datasets. The `make_mask` function allows us to create a mask using the flag labels (e.g. \"wet\" or \"dry\") rather than the binary numbers we used above. For more details on how to do masking on WOfS, see the [Applying_WOfS_bit_masking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebook in Africa sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate query object \n",
    "query ={'group_by':'solar_day',\n",
    "        'resampling':'nearest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOfS Classifier\n",
    "\n",
    "This is where the decision tree to determine if a particular pixel is wet or not wet is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wofs classifier\n",
    "import gc\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import datacube\n",
    "# Command line tool imports\n",
    "import argparse\n",
    "import os\n",
    "import collections\n",
    "import gdal\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def wofs_classify(dataset_in, clean_mask=None, x_coord='longitude', y_coord='latitude',\n",
    "                  time_coord='time', no_data=-9999, mosaic=False, enforce_float64=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Performs WOfS algorithm on given dataset.\n",
    "    Assumption:\n",
    "      - The WOfS algorithm is defined for Landsat 5/Landsat 7\n",
    "    References:\n",
    "      - Mueller, et al. (2015) \"Water observations from space: Mapping surface water from\n",
    "        25 years of Landsat imagery across Australia.\" Remote Sensing of Environment.\n",
    "      - https://github.com/GeoscienceAustralia/eo-tools/blob/stable/eotools/water_classifier.py\n",
    "    -----\n",
    "    Inputs:\n",
    "      dataset_in (xarray.Dataset) - dataset retrieved from the Data Cube; should contain\n",
    "        coordinates: time, latitude, longitude\n",
    "        variables: blue, green, red, nir, swir1, swir2\n",
    "    x_coord, y_coord, time_coord: (str) - Names of DataArrays in `dataset_in` to use as x, y,\n",
    "        and time coordinates.\n",
    "    Optional Inputs:\n",
    "      clean_mask (nd numpy array with dtype boolean) - true for values user considers clean;\n",
    "        if user does not provide a clean mask, all values will be considered clean\n",
    "      no_data (int/float) - no data pixel value; default: -9999\n",
    "      mosaic (boolean) - flag to indicate if dataset_in is a mosaic. If mosaic = False, dataset_in\n",
    "        should have a time coordinate and wofs will run over each time slice; otherwise, dataset_in\n",
    "        should not have a time coordinate and wofs will run over the single mosaicked image\n",
    "      enforce_float64 (boolean) - flag to indicate whether or not to enforce float64 calculations;\n",
    "        will use float32 if false\n",
    "    Output:\n",
    "      dataset_out (xarray.DataArray) - wofs water classification results: 0 - not water; 1 - water\n",
    "    Throws:\n",
    "        ValueError - if dataset_in is an empty xarray.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def _band_ratio(a, b):\n",
    "        \"\"\"\n",
    "        Calculates a normalized ratio index\n",
    "        \"\"\"\n",
    "        return (a - b) / (a + b)\n",
    "\n",
    "    def _run_regression(band1, band2, band3, band4, band5, band7):\n",
    "        \"\"\"\n",
    "        Regression analysis based on Australia's training data\n",
    "        TODO: Return type\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute normalized ratio indices\n",
    "        ndi_52 = _band_ratio(band5, band2)\n",
    "        ndi_43 = _band_ratio(band4, band3)\n",
    "        ndi_72 = _band_ratio(band7, band2)\n",
    "\n",
    "        #classified = np.ones(shape, dtype='uint8')\n",
    "\n",
    "        classified = np.full(shape, no_data, dtype='uint8')\n",
    "\n",
    "        # Start with the tree's left branch, finishing nodes as needed\n",
    "\n",
    "        # Left branch\n",
    "        r1 = ndi_52 <= -0.01\n",
    "\n",
    "        r2 = band1 <= 2083.5\n",
    "        classified[r1 & ~r2] = 0  #Node 3\n",
    "\n",
    "        r3 = band7 <= 323.5\n",
    "        _tmp = r1 & r2\n",
    "        _tmp2 = _tmp & r3\n",
    "        _tmp &= ~r3\n",
    "\n",
    "        r4 = ndi_43 <= 0.61\n",
    "        classified[_tmp2 & r4] = 1  #Node 6\n",
    "        classified[_tmp2 & ~r4] = 0  #Node 7\n",
    "\n",
    "        r5 = band1 <= 1400.5\n",
    "        _tmp2 = _tmp & ~r5\n",
    "\n",
    "        r6 = ndi_43 <= -0.01\n",
    "        classified[_tmp2 & r6] = 1  #Node 10\n",
    "        classified[_tmp2 & ~r6] = 0  #Node 11\n",
    "\n",
    "        _tmp &= r5\n",
    "\n",
    "        r7 = ndi_72 <= -0.23\n",
    "        _tmp2 = _tmp & ~r7\n",
    "\n",
    "        r8 = band1 <= 379\n",
    "        classified[_tmp2 & r8] = 1  #Node 14\n",
    "        classified[_tmp2 & ~r8] = 0  #Node 15\n",
    "\n",
    "        _tmp &= r7\n",
    "\n",
    "        r9 = ndi_43 <= 0.22\n",
    "        classified[_tmp & r9] = 1  #Node 17\n",
    "        _tmp &= ~r9\n",
    "\n",
    "        r10 = band1 <= 473\n",
    "        classified[_tmp & r10] = 1  #Node 19\n",
    "        classified[_tmp & ~r10] = 0  #Node 20\n",
    "\n",
    "        # Left branch complete; cleanup\n",
    "        del r2, r3, r4, r5, r6, r7, r8, r9, r10\n",
    "        gc.collect()\n",
    "\n",
    "        # Right branch of regression tree\n",
    "        r1 = ~r1\n",
    "\n",
    "        r11 = ndi_52 <= 0.23\n",
    "        _tmp = r1 & r11\n",
    "\n",
    "        r12 = band1 <= 334.5\n",
    "        _tmp2 = _tmp & ~r12\n",
    "        classified[_tmp2] = 0  #Node 23\n",
    "\n",
    "        _tmp &= r12\n",
    "\n",
    "        r13 = ndi_43 <= 0.54\n",
    "        _tmp2 = _tmp & ~r13\n",
    "        classified[_tmp2] = 0  #Node 25\n",
    "\n",
    "        _tmp &= r13\n",
    "\n",
    "        r14 = ndi_52 <= 0.12\n",
    "        _tmp2 = _tmp & r14\n",
    "        classified[_tmp2] = 1  #Node 27\n",
    "\n",
    "        _tmp &= ~r14\n",
    "\n",
    "        r15 = band3 <= 364.5\n",
    "        _tmp2 = _tmp & r15\n",
    "\n",
    "        r16 = band1 <= 129.5\n",
    "        classified[_tmp2 & r16] = 1  #Node 31\n",
    "        classified[_tmp2 & ~r16] = 0  #Node 32\n",
    "\n",
    "        _tmp &= ~r15\n",
    "\n",
    "        r17 = band1 <= 300.5\n",
    "        _tmp2 = _tmp & ~r17\n",
    "        _tmp &= r17\n",
    "        classified[_tmp] = 1  #Node 33\n",
    "        classified[_tmp2] = 0  #Node 34\n",
    "\n",
    "        _tmp = r1 & ~r11\n",
    "\n",
    "        r18 = ndi_52 <= 0.34\n",
    "        classified[_tmp & ~r18] = 0  #Node 36\n",
    "        _tmp &= r18\n",
    "\n",
    "        r19 = band1 <= 249.5\n",
    "        classified[_tmp & ~r19] = 0  #Node 38\n",
    "        _tmp &= r19\n",
    "\n",
    "        r20 = ndi_43 <= 0.45\n",
    "        classified[_tmp & ~r20] = 0  #Node 40\n",
    "        _tmp &= r20\n",
    "\n",
    "        r21 = band3 <= 364.5\n",
    "        classified[_tmp & ~r21] = 0  #Node 42\n",
    "        _tmp &= r21\n",
    "\n",
    "        r22 = band1 <= 129.5\n",
    "        classified[_tmp & r22] = 1  #Node 44\n",
    "        classified[_tmp & ~r22] = 0  #Node 45\n",
    "\n",
    "        # Completed regression tree\n",
    "\n",
    "        return classified\n",
    "\n",
    "    # Default to masking nothing.\n",
    "    if clean_mask is None:\n",
    "        clean_mask = create_default_clean_mask(dataset_in)\n",
    "    \n",
    "    # Extract dataset bands needed for calculations\n",
    "    #blue = dataset_in.blue\n",
    "    #green = dataset_in.green\n",
    "    #red = dataset_in.red\n",
    "    #nir = dataset_in.nir\n",
    "    #swir1 = dataset_in.swir_1\n",
    "    #swir2 = dataset_in.swir_2\n",
    "    blue = dataset_in.B02\n",
    "    green = dataset_in.B03\n",
    "    red = dataset_in.B04\n",
    "    nir = dataset_in.B08\n",
    "    swir1 = dataset_in.B11\n",
    "    swir2 = dataset_in.B12\n",
    "\n",
    "    # Enforce float calculations - float64 if user specified, otherwise float32 will do\n",
    "    dtype = blue.values.dtype  # This assumes all dataset bands will have\n",
    "    # the same dtype (should be a reasonable\n",
    "    # assumption)\n",
    "\n",
    "    if enforce_float64:\n",
    "        if dtype != 'float64':\n",
    "            blue.values = blue.values.astype('float64')\n",
    "            green.values = green.values.astype('float64')\n",
    "            red.values = red.values.astype('float64')\n",
    "            nir.values = nir.values.astype('float64')\n",
    "            swir1.values = swir1.values.astype('float64')\n",
    "            swir2.values = swir2.values.astype('float64')\n",
    "    else:\n",
    "        if dtype == 'float64':\n",
    "            pass\n",
    "        elif dtype != 'float32':\n",
    "            blue.values = blue.values.astype('float32')\n",
    "            green.values = green.values.astype('float32')\n",
    "            red.values = red.values.astype('float32')\n",
    "            nir.values = nir.values.astype('float32')\n",
    "            swir1.values = swir1.values.astype('float32')\n",
    "            swir2.values = swir2.values.astype('float32')\n",
    "\n",
    "    shape = blue.values.shape\n",
    "    #print('decision time!')\n",
    "    classified = _run_regression(blue.values, green.values, red.values, nir.values, swir1.values, swir2.values)\n",
    "\n",
    "    classified_clean = np.full(classified.shape, no_data, dtype='float64')\n",
    "    classified_clean[clean_mask] = classified[clean_mask]  # Contains data for clear pixels\n",
    "\n",
    "    # Create xarray of data\n",
    "    x_coords = dataset_in[x_coord]\n",
    "    y_coords = dataset_in[y_coord]\n",
    "\n",
    "    time = None\n",
    "    coords = None\n",
    "    dims = None\n",
    "\n",
    "    if mosaic:\n",
    "        coords = [y_coords, x_coords]\n",
    "        dims = [y_coord, x_coord]\n",
    "    else:\n",
    "        time_coords = dataset_in[time_coord]\n",
    "        coords = [time_coords, y_coords, x_coords]\n",
    "        dims = [time_coord, y_coord, x_coord]\n",
    "\n",
    "    data_array = xr.DataArray(classified_clean, coords=coords, dims=dims)\n",
    "\n",
    "    if mosaic:\n",
    "        dataset_out = xr.Dataset({'wofs': data_array},\n",
    "                                 coords={y_coord: y_coords, x_coord: x_coords})\n",
    "    else:\n",
    "        dataset_out = xr.Dataset(\n",
    "            {'wofs': data_array},\n",
    "            coords={time_coord: time_coords, y_coord: y_coords, x_coord: x_coords})\n",
    "\n",
    "    return dataset_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to query WOfS database according to the first five days before and after of each calendar month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wofs_for_point(index, row, input_data, query, results_wet, results_clear):\n",
    "    dc = datacube.Datacube(app='WOfS_accuracy')\n",
    "    #get the month value for each index\n",
    "    month = input_data.loc[index]['MONTH'] \n",
    "    #get the value for time including year, month, start date and end date \n",
    "    timeYM = '2018-'+f'{month:02d}'\n",
    "    # time needs changing for S2\n",
    "    #start_date = np.datetime64(timeYM) - np.timedelta64(5,'D')\n",
    "    #end_date = np.datetime64(timeYM) + np.timedelta64(5,'D')\n",
    "    start_date = np.datetime64(timeYM) #- np.timedelta64(5,'D')\n",
    "    end_date = np.datetime64(timeYM) + np.timedelta64(5,'D')\n",
    "    time = (str(start_date),str(end_date))\n",
    "    \n",
    "    plot_id = input_data.loc[index]['PLOT_ID']\n",
    "    #having the original query as it is \n",
    "    dc_query = deepcopy(query) \n",
    "    geom = geometry.Geometry(input_data.geometry.values[index].__geo_interface__,  geometry.CRS('EPSG:6933'))\n",
    "    q = {\"geopolygon\":geom}\n",
    "    t = {\"time\":time} \n",
    "    #updating the query\n",
    "    dc_query.update(t)\n",
    "    dc_query.update(q)\n",
    "    bands = ['blue', 'green', 'red', 'nir', 'swir_1', 'swir_2', 'mask']\n",
    "    # get the S2 data\n",
    "    #print('gets thems s2s')\n",
    "    # try using load not load ard\n",
    "    s2_data = dc.load(product='s2_l2a',\n",
    "                       #measurements=bands,\n",
    "                       y = (input_data.geometry.y[index], input_data.geometry.y[index]),\n",
    "                       x =(input_data.geometry.x[index], input_data.geometry.x[index]),\n",
    "                       crs = 'EPSG:6933',\n",
    "                       time=time,\n",
    "                       output_crs = 'EPSG:6933',\n",
    "                       resolution=(-10,10))\n",
    "    #s2_data = load_ard(dc=dc,\n",
    "    #                   products=['s2_l2a'],\n",
    "    #                   measurements=bands,\n",
    "    #                   y = (input_data.geometry.y[index], input_data.geometry.y[index]),\n",
    "    #                   x =(input_data.geometry.x[index], input_data.geometry.x[index]),\n",
    "    #                   crs = 'EPSG:6933',\n",
    "    #                   time=time,\n",
    "    #                   output_crs = 'EPSG:6933',\n",
    "    #                   resolution=(-10,10))\n",
    "    #print('tasty datas yum yum')\n",
    "    if not 'B02' in s2_data: #need to get in here and make it right, run on s2_data, if not red in s2_data then pass (eg) then run the classifier\n",
    "        pass\n",
    "    else:\n",
    "        # run the classifier\n",
    "        clean_mask = np.isin(s2_data['SCL'], [4,5,6,7,11])\n",
    "        #clean_mask = s2_data.to_array().isnull().any(dim='band')\n",
    "        #print('mask time woot woot')\n",
    "        # change this to mask out the nans\n",
    "        # clean_mask for nans ds.to_array().isnull().any(dim='band')\n",
    "        s2_wofs = wofs_classify(s2_data, \n",
    "                              clean_mask=clean_mask, \n",
    "                              no_data=np.nan, \n",
    "                              x_coord='x', \n",
    "                              y_coord='y')\n",
    "        #print('omg s2 wofs')\n",
    "        wofls=s2_wofs['wofs']\n",
    "        #Define a mask for dry and clear pixels\n",
    "        #wofl = bin bit mask\n",
    "        #s2_wofs.wofs==1\n",
    "        #wofl_wetnocloud = s2_wofs.wofs==1\n",
    "        wofl_wetnocloud = (wofls==1) \n",
    "        wofl_drynocloud = (wofls==0)\n",
    "        clear = (wofl_wetnocloud | wofl_drynocloud).all(dim=['x','y']).values\n",
    "        #record the total number of clear observations for each point in each month and use it to filter out month with no valid data\n",
    "        n_clear = clear.sum()  \n",
    "        #condition to identify whether WOfS seen water in specific month for a particular location \n",
    "        if n_clear > 0:\n",
    "            wet = wofl_wetnocloud.isel(time=clear).max().values  \n",
    "        else:\n",
    "            wet = 0 \n",
    "        #updating results for both wet and clear observations \n",
    "        #print('wet/not wet')\n",
    "        results_wet.update({str(int(plot_id))+\"_\"+str(month) : int(wet)})\n",
    "        results_clear.update({str(int(plot_id))+\"_\"+str(month) : int(n_clear)})        \n",
    "        \n",
    "        return time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for parallel processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_fun(input_data, query, ncpus):\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    results_wet = manager.dict()\n",
    "    results_clear = manager.dict()\n",
    "   \n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(input_data))\n",
    "        \n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in input_data.iterrows():\n",
    "            #print('in the pool, index',index)\n",
    "            pool.apply_async(get_wofs_for_point,\n",
    "                                 [index,\n",
    "                                 row,\n",
    "                                 input_data,\n",
    "                                 query,\n",
    "                                 results_wet,\n",
    "                                 results_clear], callback=update)\n",
    "        #print('leaving pool')\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "        \n",
    "    return results_wet, results_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the for loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2018-08', '2018-08-06')\n",
      "('2018-10', '2018-10-06')\n",
      "('2018-01', '2018-01-06')\n",
      "('2018-02', '2018-02-06')\n",
      "('2018-03', '2018-03-06')\n"
     ]
    }
   ],
   "source": [
    "results_wet_test = dict()\n",
    "results_clear_test = dict()\n",
    "\n",
    "for index, row in input_data[0:5].iterrows():\n",
    "    time = get_wofs_for_point(index, row, input_data, query, results_wet_test, results_clear_test)\n",
    "    print(time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point-based query and parallel processing on WOfS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8724/8724 [1:01:11<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this takes a while longer with S2 than with landsat, grab a tea or coffee while you wait?\n",
    "# takes ~1min 20sec for 10 on 2 cpus \n",
    "wet, clear = _parallel_fun(input_data, query, ncpus=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the final table with both CEO labels and WOfS class Wet and clear observations \n",
    "wetdf = pd.DataFrame.from_dict(wet, orient = 'index')\n",
    "cleardf = pd.DataFrame.from_dict(clear,orient='index')\n",
    "df2 = wetdf.merge(cleardf, left_index=True, right_index=True)\n",
    "df2 = df2.rename(columns={'0_x':'CLASS_WET','0_y':'CLEAR_OBS'})\n",
    "#split the index (which is plotid + month) into seperate columns\n",
    "for index, row in df2.iterrows():\n",
    "    df2.at[index,'PLOT_ID'] = index.split('_')[0] +'.0'\n",
    "    df2.at[index,'MONTH'] = index.split('_')[1]\n",
    "#reset the index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "#convert plot id and month to str to help with matching\n",
    "input_data['PLOT_ID'] = input_data.PLOT_ID.astype(str)\n",
    "input_data['MONTH']= input_data.MONTH.astype(str)\n",
    "# merge both dataframe at locations where plotid and month match\n",
    "final_df = pd.merge(input_data, df2, on=['PLOT_ID','MONTH'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8724, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining the shape of final table \n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 198)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of rows in the final table with NaN values in class_wet and clear observation (Optional)\n",
    "#This part is to test the parallel processig function returns identicial results each time that it runs \n",
    "countA = final_df[\"CLASS_WET\"].isna().sum()\n",
    "countB = final_df[\"CLEAR_OBS\"].isna().sum()\n",
    "countA, countB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(('../Results/WOfS_Assessment/Point_Based/Institutions/AGRYHMET_PointBased_5D_S2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.3\n"
     ]
    }
   ],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** September 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`WOfS`, :index:`fractional cover`, :index:`deafrica_plotting`, :index:`deafrica_datahandling`, :index:`display_map`, :index:`wofs_fuser`, :index:`WOFL`, :index:`masking`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "9e3fa49adf8c4170abfcd954c2ec045a": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "dc642f11c1fb492ca419b0ed6fc4f8c3": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
