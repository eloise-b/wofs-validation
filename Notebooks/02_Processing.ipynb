{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point-based and Parallel Processing Water Observations from Space (WOfS) Product in Africa  <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:**\n",
    "[ga_ls8c_wofs_2](https://explorer.digitalearth.africa/ga_ls8c_wofs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description \n",
    "The [Water Observations from Space (WOfS)](https://www.ga.gov.au/scientific-topics/community-safety/flood/wofs/about-wofs) is a derived product from Landsat 8 satellite observations as part of provisional Landsat 8 Collection 2 surface reflectance and shows surface water detected in Africa.\n",
    "Individual water classified images are called Water Observation Feature Layers (WOFLs), and are created in a 1-to-1 relationship with the input satellite data. \n",
    "Hence there is one WOFL for each satellite dataset processed for the occurrence of water.\n",
    "\n",
    "The data in a WOFL is stored as a bit field. This is a binary number, where each digit of the number is independantly set or not based on the presence (1) or absence (0) of a particular attribute (water, cloud, cloud shadow etc). In this way, the single decimal value associated to each pixel can provide information on a variety of features of that pixel. \n",
    "For more information on the structure of WOFLs and how to interact with them, see [Water Observations from Space](../Datasets/Water_Observations_from_Space.ipynb) and [Applying WOfS bitmasking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebooks.\n",
    "\n",
    "This notebook explains how you can query WOfS product for each collected validation points in Africa based on point-based sampling approach. \n",
    "\n",
    "The notebook demonstrates how to:\n",
    "\n",
    "1. Load validation points for each partner institutions following cleaning stage described in \n",
    "2. Query WOFL data for validation points and capture available WOfS defined class using point-based sampling and multiprocessing functionality\n",
    "3. Extract a LUT for each point that contains both information for validation points and WOfS class as well number of clear observation in each month \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "from datacube.utils import masking, geometry \n",
    "import sys\n",
    "import os\n",
    "import rasterio\n",
    "import xarray\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import scipy, scipy.ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #this will suppress the warnings for multiple UTM zones in your AOI \n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score  \n",
    "from deafrica_plotting import map_shapefile,display_map, rgb\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_datahandling import wofs_fuser, mostcommon_crs,load_ard,deepcopy\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse validation points collected by each partner institution, we need to obtain WOfS surface water observation data that corresponds with the labelled input data locations. \n",
    "- Path2csv: the path to CEO validation points labelled by each partner institutions in Africa \n",
    "- ValPoints: CEO validation points labelled by each partner institutions in Africa in ESRI shapefile format \n",
    "- Path: Direct path to the ESRI shapefile in case that the shapefile in available\n",
    "- input_data: geopandas datafram for CEO validation points labelled by each partner institutions in Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Note: Run the following three cells in case that you dont have a ESRI shapefile for validation points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2csv = '../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.csv'\n",
    "df = pd.read_csv(path2csv,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometries = [Point(xy) for xy in zip(df.LON, df.LAT)]\n",
    "crs = {'init': 'epsg:4326'} \n",
    "ValPoints = GeoDataFrame(df, crs=crs, geometry=geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValPoints.to_file(filename='../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.shp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Note: In case that you have ESRI shapefile for validation points, please continute from this point onward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/Processed/AGRYHMET/AGRYHMET_ValidationPoints.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed_ 0', 'PLOT_ID', 'LON', 'LAT', 'FLAGGED', 'ANALYSES',\n",
       "       'SENTINEL2Y', 'STARTDATE', 'ENDDATE', 'WATER', 'NO_WATER', 'BAD_IMAGE',\n",
       "       'NOT_SURE', 'CLASS', 'COMMENT', 'MONTH', 'WATERFLAG', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the table and converting CRS to metric\n",
    "input_data = gpd.read_file(path).to_crs('epsg:6933')  \n",
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= input_data.drop(['Unnamed_ 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8724, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the size of the input data \n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample WOfS at the ground truth coordinates\n",
    "To load WOFL data, we can first create a re-usable query as below that will define two particular items, `group_by` solar day, ensuring that the data between scenes is combined correctly. The second parameter is `resampling` method that is set to be nearest. This query will later be updated in the script for other parameters to conduct WOfS query. the time period we are interested in, as well as other important parameters that are used to correctly load the data. \n",
    "\n",
    "We can convert the WOFL bit field into a binary array containing True and False values. This allows us to use the WOFL data as a mask that can be applied to other datasets. The `make_mask` function allows us to create a mask using the flag labels (e.g. \"wet\" or \"dry\") rather than the binary numbers we used above. For more details on how to do masking on WOfS, see the [Applying_WOfS_bit_masking](../Frequently_used_code/Applying_WOfS_bitmasking.ipynb) notebook in Africa sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate query object \n",
    "query ={'group_by':'solar_day',\n",
    "        'resampling':'nearest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to query WOfS database according to the first five days before and after of each calendar month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wofs_for_point(index, row, input_data, query, results_wet, results_clear):\n",
    "    dc = datacube.Datacube(app='WOfS_accuracy')\n",
    "    #get the month value for each index\n",
    "    month = input_data.loc[index]['MONTH'] \n",
    "    #get the value for time including year, month, start date and end date \n",
    "    timeYM = '2018-'+f'{month:02d}'\n",
    "    start_date = np.datetime64(timeYM) - np.timedelta64(5,'D')\n",
    "    end_date = np.datetime64(timeYM) + np.timedelta64(5,'D')\n",
    "    time = (str(start_date),str(end_date))\n",
    "    \n",
    "    plot_id = input_data.loc[index]['PLOT_ID']\n",
    "    #having the original query as it is \n",
    "    dc_query = deepcopy(query) \n",
    "    geom = geometry.Geometry(input_data.geometry.values[index].__geo_interface__,  geometry.CRS('EPSG:6933'))\n",
    "    q = {\"geopolygon\":geom}\n",
    "    t = {\"time\":time} \n",
    "    #updating the query\n",
    "    dc_query.update(t)\n",
    "    dc_query.update(q)\n",
    "    #loading landsat-8 WOfs product and set the values for x and y (point-based) and also (window-based)\n",
    "    wofls = dc.load(product =\"ga_ls8c_wofs_2\",\n",
    "                    y = (input_data.geometry.y[index], input_data.geometry.y[index]),\n",
    "                    x =(input_data.geometry.x[index], input_data.geometry.x[index]),\n",
    "                    #y = (input_data.geometry.y[index] - 30.5, input_data.geometry.y[index] + 30.5), # setting x and y coordinates based on 3*3 pixel window-based query \n",
    "                    #x =(input_data.geometry.x[index] - 30.5, input_data.geometry.x[index] + 30.5),\n",
    "                    crs = 'EPSG:6933',\n",
    "                    time=time,\n",
    "                    output_crs = 'EPSG:6933',\n",
    "                    resolution=(-30,30))\n",
    "    #exclude the records that wofl return as empty for water \n",
    "    if not 'water' in wofls:\n",
    "        pass\n",
    "    else:\n",
    "        #Define a mask for wet and clear pixels \n",
    "        wet_nocloud = {\"water_observed\":True, \"cloud_shadow\":False, \"cloud\":False,\"nodata\":False}\n",
    "        #Define a mask for dry and clear pixels \n",
    "        dry_nocloud = {\"water_observed\":False, \"cloud_shadow\":False, \"cloud\":False, \"nodata\":False}\n",
    "        wofl_wetnocloud = masking.make_mask(wofls, **wet_nocloud).astype(int) \n",
    "        wofl_drynocloud = masking.make_mask(wofls, **dry_nocloud).astype(int)\n",
    "        clear = (wofl_wetnocloud | wofl_drynocloud).water.all(dim=['x','y']).values\n",
    "        #record the total number of clear observations for each point in each month and use it to filter out month with no valid data\n",
    "        n_clear = clear.sum()  \n",
    "        #condition to identify whether WOfS seen water in specific month for a particular location \n",
    "        if n_clear > 0:\n",
    "            wet = wofl_wetnocloud.isel(time=clear).water.max().values  \n",
    "        else:\n",
    "            wet = 0 \n",
    "        #updating results for both wet and clear observations \n",
    "        results_wet.update({str(int(plot_id))+\"_\"+str(month) : int(wet)})\n",
    "        results_clear.update({str(int(plot_id))+\"_\"+str(month) : int(n_clear)})\n",
    "        \n",
    "        return time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for parallel processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parallel_fun(input_data, query, ncpus):\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    results_wet = manager.dict()\n",
    "    results_clear = manager.dict()\n",
    "   \n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(input_data))\n",
    "        \n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in input_data.iterrows():\n",
    "            pool.apply_async(get_wofs_for_point,\n",
    "                                 [index,\n",
    "                                 row,\n",
    "                                 input_data,\n",
    "                                 query,\n",
    "                                 results_wet,\n",
    "                                 results_clear], callback=update)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "        \n",
    "    return results_wet, results_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the for loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "('2018-09-26', '2018-10-06')\n",
      "('2017-12-27', '2018-01-06')\n",
      "('2018-01-27', '2018-02-06')\n",
      "('2018-02-24', '2018-03-06')\n",
      "None\n",
      "None\n",
      "('2018-11-26', '2018-12-06')\n",
      "('2018-03-27', '2018-04-06')\n",
      "None\n",
      "('2018-08-27', '2018-09-06')\n",
      "('2018-11-26', '2018-12-06')\n",
      "('2017-12-27', '2018-01-06')\n",
      "('2018-01-27', '2018-02-06')\n"
     ]
    }
   ],
   "source": [
    "results_wet_test = dict()\n",
    "results_clear_test = dict()\n",
    "\n",
    "for index, row in input_data[0:14].iterrows():\n",
    "    time = get_wofs_for_point(index, row, input_data, query, results_wet_test, results_clear_test)\n",
    "    print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point-based query and parallel processing on WOfS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:01,  7.04it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:00<00:01,  6.35it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:00<00:00,  7.19it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:00<00:00,  8.83it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.44it/s][A\n"
     ]
    }
   ],
   "source": [
    "wet, clear = _parallel_fun(input_data, query, ncpus=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the final table with both CEO labels and WOfS class Wet and clear observations \n",
    "wetdf = pd.DataFrame.from_dict(wet, orient = 'index')\n",
    "cleardf = pd.DataFrame.from_dict(clear,orient='index')\n",
    "df2 = wetdf.merge(cleardf, left_index=True, right_index=True)\n",
    "df2 = df2.rename(columns={'0_x':'CLASS_WET','0_y':'CLEAR_OBS'})\n",
    "#split the index (which is plotid + month) into seperate columns\n",
    "for index, row in df2.iterrows():\n",
    "    df2.at[index,'PLOT_ID'] = index.split('_')[0] +'.0'\n",
    "    df2.at[index,'MONTH'] = index.split('_')[1]\n",
    "#reset the index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "#convert plot id and month to str to help with matching\n",
    "input_data['PLOT_ID'] = input_data.PLOT_ID.astype(str)\n",
    "input_data['MONTH']= input_data.MONTH.astype(str)\n",
    "# merge both dataframe at locations where plotid and month match\n",
    "final_df = pd.merge(input_data, df2, on=['PLOT_ID','MONTH'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8724, 19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining the shape of final table \n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8717, 8717)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of rows in the final table with NaN values in class_wet and clear observation (Optional)\n",
    "#This part is to test the parallel processig function returns identicial results each time that it runs \n",
    "countA = final_df[\"CLASS_WET\"].isna().sum()\n",
    "countB = final_df[\"CLEAR_OBS\"].isna().sum()\n",
    "countA, countB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(('../../Results/WOfS_Assessment/Point_Based/Institutions/AGRYHMET_PointBased_5D.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.3\n"
     ]
    }
   ],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** September 2020\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DE Africa User Guide's [Tags Index](https://) (placeholder as this does not exist yet)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**:  :index:`WOfS`, :index:`fractional cover`, :index:`deafrica_plotting`, :index:`deafrica_datahandling`, :index:`display_map`, :index:`wofs_fuser`, :index:`WOFL`, :index:`masking`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "9e3fa49adf8c4170abfcd954c2ec045a": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "dc642f11c1fb492ca419b0ed6fc4f8c3": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
